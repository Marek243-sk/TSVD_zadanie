{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd9a5153-1771-4307-8385-d245ac8c80e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised_learning.ipynb\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Inicializácia SparkSession\n",
    "# ------------------------------------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"zadanieTSVD\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d0d091-e9d9-48ff-b852-63d8472a95bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- K-means clustering ---\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2459842-3f07-4af3-815e-5b3d4db2ec3b",
   "metadata": {},
   "source": [
    "MERGED DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28427851-78e9-48e4-80fe-cb37c193c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analýza zlúčeného datasetu (final dataset) ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Zlúčený dataset: Spájame Accidents, Casualties a Vehicles\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n=== Analýza zlúčeného datasetu (final dataset) ===\\n\")\n",
    "# Predpokladáme, že vo všetkých troch datasetoch existuje spoločný kľúč: Accident_Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df407ab-ad63-45a7-8d3f-3103d15a3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Načítanie vyčistených datasetov\n",
    "accidents_clean = spark.read.csv(\"Accidents.csv\", header=True, inferSchema=True)\n",
    "casualties_clean = spark.read.csv(\"casualties.csv\", header=True, inferSchema=True)\n",
    "vehicles_clean = spark.read.csv(\"vehicles.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37fd62d3-9349-4f47-a299-8601f2628440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Počet záznamov zlúčeného datasetu: 4287593\n",
      "Počet stĺpcov zlúčeného datasetu: 67\n"
     ]
    }
   ],
   "source": [
    "# Ak sú stĺpce, ako Vehicle_Reference v Casualties či Vehicles, spracujeme podľa toho (tiež upravujeme názvy ak je potrebné)\n",
    "# V našom príklade predpokladáme, že kľúčom je \"Accident_Index\" a ďalšie konfliktné stĺpce boli odstránené.\n",
    "\n",
    "# Zlúčenie datasetov – použijeme inner join, aby sme získali záznamy, ktoré sa vyskytujú vo všetkých troch\n",
    "merged = accidents_clean.join(casualties_clean, on=\"Accident_Index\", how=\"inner\")\\\n",
    "                        .join(vehicles_clean, on=\"Accident_Index\", how=\"inner\")\n",
    "\n",
    "print(\"Počet záznamov zlúčeného datasetu:\", merged.count())\n",
    "print(\"Počet stĺpcov zlúčeného datasetu:\", len(merged.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d52a2f8-7281-4bca-921a-56f61b0613ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre clustering a PCA si vyberieme niekoľko numerických atribútov zo zlúčeného datasetu.\n",
    "# Príklad – môžu to byť niektoré atribúty z nehôd, obetí a vozidiel:\n",
    "merged_numeric_cols = [\"Number_of_Vehicles\", \"Number_of_Casualties\", \"Speed_limit\", \n",
    "                         \"Age_of_Casualty\", \"Age_of_Driver\", \"Engine_Capacity_(CC)\", \n",
    "                         \"Age_of_Vehicle\", \"Driver_IMD_Decile\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe5de161-cd3d-4eb5-b9d1-d8c42c98acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_merged = VectorAssembler(inputCols=merged_numeric_cols, outputCol=\"features\")\n",
    "merged_features = assembler_merged.transform(merged).select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3e37317-f6b3-4b26-ac55-a361eb673f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- K-means clustering pre zlúčený dataset ---\n",
    "k_merged = 5\n",
    "kmeans_merged = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=k_merged, seed=42)\n",
    "model_merged = kmeans_merged.fit(merged_features)\n",
    "pred_merged = model_merged.transform(merged_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff87be7-7f6f-46aa-a54d-c08e9a6b4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uloženie K-means modelu pre zlúčený dataset\n",
    "model_merged.save(\"kmeans_merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63eb1d5-9cd3-4ce4-a031-996d1233475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Načítanie K-means modelu\n",
    "loaded_kmeans_model = KMeansModel.load(\"kmeans_merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "686e7d27-9ed8-44a3-94d2-2cecb04715e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset: Silhouette Score (k=5): 0.838\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Definuj evaluator\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"cluster\", \n",
    "                                metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "\n",
    "# Teraz môžeš vyhodnotiť klastrové predikcie\n",
    "silhouette_merged = evaluator.evaluate(pred_merged)\n",
    "print(f\"Merged dataset: Silhouette Score (k={k_merged}): {silhouette_merged:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68467a1e-1be3-4b8e-834f-37b25a5dd6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged dataset: Výsledok PCA (prvých 10 riadkov):\n",
      "+-----------------------------------------+\n",
      "|pcaFeatures                              |\n",
      "+-----------------------------------------+\n",
      "|[0.8255879943794742,-82.595504545656]    |\n",
      "|[-8268.102159263108,-44.13235566643481]  |\n",
      "|[-1769.1256617266208,-56.36384894896506] |\n",
      "|[0.9051250591866675,-47.08214677762155]  |\n",
      "|[-4266.107127948527,-46.83122695306251]  |\n",
      "|[-5343.148895968204,-52.33780585823041]  |\n",
      "|[-5343.148895968204,-52.33780585823041]  |\n",
      "|[-1124.0721298394446,-29.194579938628756]|\n",
      "|[-124.06955057074252,-31.43686545773735] |\n",
      "|[-1360.152634241881,-86.11268917196479]  |\n",
      "+-----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Merged dataset: Vysvetlená variabilita: [0.9996535840941086,0.00016729112013977863]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "# --- PCA pre zlúčený dataset ---\n",
    "pca_merged = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "pca_model_merged = pca_merged.fit(merged_features)\n",
    "merged_pca_result = pca_model_merged.transform(merged_features)\n",
    "print(\"\\nMerged dataset: Výsledok PCA (prvých 10 riadkov):\")\n",
    "merged_pca_result.select(\"pcaFeatures\").show(10, truncate=False)\n",
    "print(f\"Merged dataset: Vysvetlená variabilita: {pca_model_merged.explainedVariance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150094c-a2cb-41b1-bc1c-2b36ef1f05e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085a653-8ba8-4561-8578-a52e2f85ba15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588cc333-b9d1-4e44-a19f-32c1dd1ecee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "878c4b9c-917c-45db-a80f-7f8935df636c",
   "metadata": {},
   "source": [
    "ASOCIAČNE -> ZLUČENY DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73c7b778-45fc-4504-bfa2-199024e10c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Asociačné pravidlá na zlúčenom datasete ---\n",
    "# Pre asociačné pravidlá chceme pracovať s transakčnými dátami.\n",
    "# Pre tento príklad vyberieme niekoľko kategóriových atribútov, ktoré premeníme na zoznam položiek.\n",
    "# Uprav si zoznam atribútov podľa aktuálnych dát.\n",
    "from pyspark.sql.functions import array, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aa90e75-2f60-4774-8e47-dd61e9a160fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"Police_Force\", \"Accident_Severity\", \"Road_Type\", \"Day_of_Week\"]\n",
    "# Over, či tieto stĺpce existujú v zlúčenom datasete. Potom vytvoríme nový stĺpec \"items\"\n",
    "merged_transactions = merged.select(\"Accident_Index\", *categorical_cols)\n",
    "merged_transactions = merged_transactions.withColumn(\"items\", array(*[col(c).cast(\"string\") for c in categorical_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c0bc547-9360-45ac-b32d-ce4947b26391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Asociačné pravidlá (Merged dataset):\n",
      "+----------+----------+------------------+------------------+-------------------+\n",
      "|antecedent|consequent|confidence        |lift              |support            |\n",
      "+----------+----------+------------------+------------------+-------------------+\n",
      "|[7]       |[3]       |0.8635150789232473|0.9703921479055384|0.14325963308550974|\n",
      "|[7]       |[6]       |0.7130632454408962|0.9157503336606166|0.11829924155580998|\n",
      "|[4]       |[3]       |0.882270809214667 |0.9914692707575478|0.1526817027642316 |\n",
      "|[4]       |[6]       |0.7412367535455282|0.9519321164297656|0.1282750018483564 |\n",
      "|[4, 6]    |[3]       |0.8517121189255824|0.9571283381779087|0.10925337362944663|\n",
      "|[1, 3]    |[6]       |0.6441522722838056|0.8272515264861356|0.1616888543292239 |\n",
      "|[1, 6]    |[3]       |0.819651120722067 |0.921099157367888 |0.1616888543292239 |\n",
      "|[6]       |[3]       |0.8659153939030151|0.9730895493357151|0.6742584942180846 |\n",
      "|[2, 3]    |[6]       |0.6335463525508446|0.8136308910146896|0.10973686168439961|\n",
      "|[2]       |[3]       |0.6348501854569918|0.7134254515069065|0.17321047030350128|\n",
      "|[2]       |[6]       |0.7471031694809341|0.9594660517217442|0.2038372112278381 |\n",
      "|[5]       |[3]       |0.8826001378634487|0.9918393603398357|0.14871700742118013|\n",
      "|[5]       |[6]       |0.7273681092498623|0.9341213321516841|0.12256060684864445|\n",
      "|[1]       |[3]       |0.8630366463597019|0.9698544998501673|0.2510102987853558 |\n",
      "|[1]       |[6]       |0.6782483458658073|0.8710393545068171|0.19726545873174062|\n",
      "|[5, 3]    |[6]       |0.6994266339208141|0.8982375371038482|0.10401663590737274|\n",
      "|[3]       |[6]       |0.7577113178657041|0.9730895493357151|0.6742584942180846 |\n",
      "|[4, 3]    |[6]       |0.7155629761226451|0.9189606087947078|0.10925337362944663|\n",
      "|[5, 6]    |[3]       |0.8486955032445908|0.95373835664599  |0.10401663590737274|\n",
      "+----------+----------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_distinct\n",
    "\n",
    "# Predpokladám, že už máš DataFrame merged_transactions so stĺpcom \"items\"\n",
    "merged_transactions = merged_transactions.withColumn(\"items\", array_distinct(\"items\"))\n",
    "\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "fpgrowth = FPGrowth(itemsCol=\"items\", minSupport=0.1, minConfidence=0.6)\n",
    "fpModel = fpgrowth.fit(merged_transactions)\n",
    "print(\"\\nAsociačné pravidlá (Merged dataset):\")\n",
    "fpModel.associationRules.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c070402b-a509-4a14-bc90-109e71cf2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uloženie FPGrowth modelu pre asociačné pravidlá\n",
    "fpModel.save(\"fpgrowth_merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f649628-31bf-47c8-82f5-ec563fb3c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Načítanie FPGrowth modelu\n",
    "loaded_fpModel = FPGrowthModel.load(\"DATA/fpgrowth_merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd410a-7163-4d33-91bc-09977aba692d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
