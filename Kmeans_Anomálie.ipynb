{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b439200a",
   "metadata": {},
   "source": [
    "### Načítanie knižníc a vytvorenie Spark session\n",
    "\n",
    "Importujú sa potrebné knižnice: `pyspark` pre prácu s veľkými dátami, `numpy` pre numerické operácie a `matplotlib` na vizualizáciu. Následne sa vytvára Spark session s názvom `zadanieTSVD`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a8f43c-d73c-459d-84c7-c2f0b1cc1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b514ed09-237c-4a4b-9d6d-f028662cce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"zadanieTSVD\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753af108",
   "metadata": {},
   "source": [
    "### Načítanie a zlúčenie datasetov\n",
    "\n",
    "Načítavajú sa dva samostatné datasety: trénovací (`train.csv`) a testovací (`test.csv`). Po ich načítaní sa spoja do jedného veľkého datasetu a zároveň vypíše počet riadkov pred a po zlúčení.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3f7df7-34ab-44ee-83c3-979b9f791891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 256657, Test rows: 170872\n",
      "Merged rows: 427529\n"
     ]
    }
   ],
   "source": [
    "## Načítanie a spojenie datasetov ##\n",
    "\n",
    "train_df = spark.read.csv(\"DATA/train.csv\", header=True, inferSchema=True)\n",
    "test_df  = spark.read.csv(\"DATA/test.csv\", header=True, inferSchema=True)\n",
    "\n",
    "selected_cols = [\"Casualty_Severity\",\"Accident_Severity\"]\n",
    "\n",
    "train_df = train_df.select(selected_cols)\n",
    "test_df = test_df.select(selected_cols)\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "print(f\"Train rows: {train_count}, Test rows: {test_count}\")\n",
    "\n",
    "data_df = train_df.unionByName(test_df)\n",
    "merged_count = data_df.count()\n",
    "print(f\"Merged rows: {merged_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15747ba",
   "metadata": {},
   "source": [
    "### Príprava atribútov pre klasifikáciu\n",
    "\n",
    "Zo zlúčeného datasetu sa odstraňuje nepotrebný stĺpec `id`, a zvyšné stĺpce sa transformujú do jedného vektora pomocou `VectorAssembler`, ktorý vytvorí nový stĺpec `features` – vstup pre model KMeans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f50659-183e-40c0-bc9b-f044fa7ce105",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Príprava atribútov ##\n",
    "\n",
    "feature_cols = data_df.columns.copy()\n",
    "if 'id' in feature_cols:\n",
    "    feature_cols.remove('id')\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_feat = assembler.transform(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba431d0",
   "metadata": {},
   "source": [
    "### Trénovanie KMeans modelu\n",
    "\n",
    "Používa sa KMeans algoritmus na rozdelenie dát do `k=3` klastrov. Výsledný model sa uloží.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986be94b-0419-471a-9067-68611e99dd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odstraňuje sa existujúci model: kmeans__model\n"
     ]
    }
   ],
   "source": [
    "## Trénovanie K-Means ##\n",
    "\n",
    "model_path = \"kmeans__model\"\n",
    "\n",
    "# Ak model existuje, zmaže sa\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Odstraňuje sa existujúci model: {model_path}\")\n",
    "    shutil.rmtree(model_path)\n",
    "\n",
    "# Trénovanie a uloženie nového modelu\n",
    "k = 3\n",
    "kmeans = KMeans(k=k, featuresCol=\"features\", predictionCol=\"prediction\", seed=1234)\n",
    "model = kmeans.fit(df_feat)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c2a81",
   "metadata": {},
   "source": [
    "### Načítanie uloženého modelu\n",
    "\n",
    "Model, ktorý bol uložený sa načíta späť pomocou `KMeansModel.load`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9d0ffc-f5d9-416a-827a-52a6ba4aa43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Načítanie modelu ##\n",
    "\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "loaded_model = KMeansModel.load(\"kmeans__model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900a3cf",
   "metadata": {},
   "source": [
    "### Detekcia anomálií\n",
    "\n",
    "Získajú sa centroidy (stredy) jednotlivých klastrov. Pomocou UDF funkcie sa vypočíta vzdialenosť každého bodu od svojho klastrového centroidu. Potom sa na základe 95. percentilu týchto vzdialeností nastaví prah, nad ktorým sa záznam považuje za anomáliu. Anomálie sa označia stĺpcom `anomaly`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d53e31f0-77fc-4e5e-a284-6cdf9c9e9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detekcia anomálií ##\n",
    "\n",
    "## Centroidy algoritmu##\n",
    "centers = model.clusterCenters()\n",
    "## UDF na výpočet vzdialenosti ##\n",
    "@udf(returnType=\"double\")\n",
    "def calc_dist_udf(features, cluster):\n",
    "    center = centers[cluster]\n",
    "    diff = np.array(features) - center\n",
    "    return float(np.sqrt(np.dot(diff, diff)))\n",
    "\n",
    "## Pridanie stĺpca \"distance\" ##\n",
    "df_dist = model.transform(df_feat) \\\n",
    "    .withColumn(\"distance\", calc_dist_udf(col(\"features\"), col(\"prediction\")))\n",
    "## 95. percentil ako prah anomálie ##\n",
    "threshold = df_dist.approxQuantile(\"distance\", [0.95], 0.0)[0]\n",
    "## Pridanie príznaku anomálie ##\n",
    "df_anom = df_dist.withColumn(\"anomaly\", col(\"distance\") > threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41051d",
   "metadata": {},
   "source": [
    "### Uloženie výsledkov do CSV\n",
    "\n",
    "Vyfiltrované výsledky bez stĺpca `features` sa spoja do jedného CSV súboru a uložia sa do `data_with__anomalies.csv` a vypíše sa použitý prah pre detekciu anomálií.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1baad5e3-c53c-4fca-84dc-a2fd17550fa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/home/jovyan/work/TSVD_zadanie-main/data_with__anomalies.csv already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Spojíme partičky do jedného súboru\u001b[39;00m\n\u001b[32m      5\u001b[39m single_df = df_anom.select(*output_cols).coalesce(\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43msingle_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata_with__anomalies.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVýsledky uložené do \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdata_with__anomalies.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. Anomálny prah = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1864\u001b[39m, in \u001b[36mDataFrameWriter.csv\u001b[39m\u001b[34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.mode(mode)\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m   1847\u001b[39m     compression=compression,\n\u001b[32m   1848\u001b[39m     sep=sep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1862\u001b[39m     lineSep=lineSep,\n\u001b[32m   1863\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1864\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_ALREADY_EXISTS] Path file:/home/jovyan/work/TSVD_zadanie-main/data_with__anomalies.csv already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04"
     ]
    }
   ],
   "source": [
    "## Uloženie výsledkov do CSV ##\n",
    "\n",
    "output_path = \"data_with__anomalies.csv\"\n",
    "\n",
    "# Ak výstupný priečinok existuje, zmaž ho\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"Odstraňuje sa existujúci priečinok: {output_path}\")\n",
    "    shutil.rmtree(output_path)\n",
    "\n",
    "# Vyber len požadované stĺpce, zredukuj partície na 1 a ulož\n",
    "output_cols = [c for c in df_anom.columns if c != 'features']\n",
    "single_df = df_anom.select(*output_cols).coalesce(1)\n",
    "single_df.write.csv(output_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ebc672",
   "metadata": {},
   "source": [
    "### Zobrazenie výsledkov klastrovania\n",
    "\n",
    "Na záver sa vypočíta a zobrazí počet záznamov v jednotlivých klastroch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ce1fe-8528-4499-8f02-931891e5bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zobrazenie výsledkov ##\n",
    "\n",
    "cluster_sizes = df_anom.groupBy(\"prediction\").count().toPandas()\n",
    "print(\"Počet záznamov v jednotlivých klastroch:\")\n",
    "print(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b69ef5-7824-4051-baf0-b0c140b09379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
